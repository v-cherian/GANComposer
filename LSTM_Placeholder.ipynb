{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM Placeholder.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ep4FnY9HVnKc"
      },
      "source": [
        "from music21 import *\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import glob\n",
        "\n",
        "# PART.1 #####################################################################\n",
        "# Process data and get it ready for LSTM run\n",
        "\n",
        "# visualizations of midi data\n",
        "midi_data = converter.parse('data/ct.38-40 no.1.mid')\n",
        "midi_data.write('text')\n",
        "midi_data.plot()\n",
        "midi_data.plot('histogram', 'pitchclass')\n",
        "\n",
        "# prepare music data\n",
        "def process_data(songs):\n",
        "    whole_data = []\n",
        "    for song in songs:\n",
        "        midi_data = converter.parse(song).flat\n",
        "        song_data = []\n",
        "        prev_offset = -1\n",
        "        for element in midi_data:\n",
        "            if isinstance(element, note.Note):\n",
        "                if element.offset != prev_offset:\n",
        "                    song_data.append([element.pitch.nameWithOctave, \n",
        "                                      element.quarterLength])\n",
        "                else:\n",
        "                    if len(song_data[-1]) < 4:\n",
        "                        song_data[-1].append(element.pitch.nameWithOctave)   \n",
        "                        song_data[-1].append(element.quarterLength)       \n",
        "                prev_offset = element.offset\n",
        "            elif isinstance(element, chord.Chord):\n",
        "                pitch_names = '.'.join(n.nameWithOctave for n in element.pitches)\n",
        "                if element.offset != prev_offset:\n",
        "                    song_data.append([pitch_names, element.quarterLength])\n",
        "                else:\n",
        "                    if len(song_data[-1]) < 4:\n",
        "                        song_data[-1].append(pitch_names)   \n",
        "                        song_data[-1].append(element.quarterLength)      \n",
        "                prev_offset = element.offset\n",
        "        for item in song_data:\n",
        "            if len(item) < 4:\n",
        "                item.append(None)\n",
        "                item.append(None)\n",
        "        whole_data.append(song_data)\n",
        "    return whole_data\n",
        "\n",
        "\n",
        "# transform data to tuple instead of list and pad songs that are shorter in\n",
        "# length with (None, None, None, None)\n",
        "def transform_data(songs):\n",
        "    max_len = 0\n",
        "    for song in songs:\n",
        "        max_len = max(max_len, len(song))\n",
        "    for song in songs:\n",
        "        for i in range(max_len - len(song)):\n",
        "            song.append([None, None, None, None])\n",
        "    transform_data = []\n",
        "    for song in songs:\n",
        "        t_song_data = []\n",
        "        for item in song:\n",
        "            t_song_data.append(tuple(item))\n",
        "        transform_data.append(t_song_data)\n",
        "    return transform_data\n",
        "\n",
        "\n",
        "# get a dictionary of the unique notes\n",
        "def get_dictionary(songs):\n",
        "    possible_combs = set(item for song in songs for item in song)\n",
        "    data_to_int = dict((v, i) for i, v in enumerate(possible_combs))\n",
        "    int_to_data = dict((i, v) for i, v in enumerate(possible_combs))\n",
        "    return data_to_int, int_to_data    \n",
        "\n",
        "songs = glob.glob('data/*.mid')\n",
        "songs_data = process_data(songs)\n",
        "songs_data = transform_data(songs_data)\n",
        "# # of songs (samples) x # of timestamps x tuple of 4 (combination of notes\n",
        "# and rhythms of both hands)\n",
        "print(\"Whole music data size:\", np.array(songs_data).shape)\n",
        "data_to_int, int_to_data = get_dictionary(songs_data)\n",
        "print(\"Number of unique notes:\", len(data_to_int))\n",
        "\n",
        "\n",
        "# PART.2 #####################################################################\n",
        "# train data which is divided into specified number of batches in LSTM\n",
        "\n",
        "# hyperparameters ############################################################\n",
        "INPUT = 100\n",
        "HIDDEN = 256\n",
        "vocab_size = len(data_to_int)\n",
        "OUTPUT = vocab_size\n",
        "LEARNING_RATE = 0.005\n",
        "BETA1 = 0.9\n",
        "BETA2 = 0.999\n",
        "BATCH_SIZE = 50\n",
        "\n",
        "# activation functions #######################################################\n",
        "def sigmoid(X):\n",
        "    return 1. / (1 + np.exp(-X))\n",
        "\n",
        "def softmax(X):\n",
        "    exp_X = np.exp(X)\n",
        "    return exp_X / np.sum(exp_X, axis=1).reshape(-1, 1)\n",
        "\n",
        "def tanh(X):\n",
        "    return np.tanh(X)\n",
        "\n",
        "def dtanh(X):\n",
        "    return 1 - X**2\n",
        "\n",
        "# LSTM #######################################################################\n",
        "def initialize_parameters():\n",
        "    parameters = {}\n",
        "    parameters['fgw'] = np.random.normal(0,0.01,(INPUT+HIDDEN,HIDDEN))\n",
        "    parameters['igw'] = np.random.normal(0,0.01,(INPUT+HIDDEN,HIDDEN))\n",
        "    parameters['ogw'] = np.random.normal(0,0.01,(INPUT+HIDDEN,HIDDEN))\n",
        "    parameters['ggw'] = np.random.normal(0,0.01,(INPUT+HIDDEN,HIDDEN))\n",
        "    parameters['how'] = np.random.normal(0,0.01,(HIDDEN,OUTPUT)) \n",
        "    return parameters\n",
        "\n",
        "\n",
        "def initialize_V(parameters):\n",
        "    V = {}\n",
        "    V['vfgw'] = np.zeros(parameters['fgw'].shape)\n",
        "    V['vigw'] = np.zeros(parameters['igw'].shape)\n",
        "    V['vogw'] = np.zeros(parameters['ogw'].shape)\n",
        "    V['vggw'] = np.zeros(parameters['ggw'].shape)\n",
        "    V['vhow'] = np.zeros(parameters['how'].shape)\n",
        "    return V\n",
        "\n",
        "\n",
        "def initialize_S(parameters):\n",
        "    S = {}\n",
        "    S['sfgw'] = np.zeros(parameters['fgw'].shape)\n",
        "    S['sigw'] = np.zeros(parameters['igw'].shape)\n",
        "    S['sogw'] = np.zeros(parameters['ogw'].shape)\n",
        "    S['sggw'] = np.zeros(parameters['ggw'].shape)\n",
        "    S['show'] = np.zeros(parameters['how'].shape)\n",
        "    return S\n",
        "\n",
        "\n",
        "def get_embeddings(batch_dataset, embeddings):\n",
        "    embedding_dataset = np.matmul(batch_dataset, embeddings)\n",
        "    return embedding_dataset\n",
        "\n",
        "\n",
        "def lstm_cell(batch_dataset, prev_activation_matrix, prev_cell_matrix, parameters):\n",
        "    concat_dataset = np.concatenate((batch_dataset, prev_activation_matrix), axis=1)\n",
        "    fa = sigmoid(np.matmul(concat_dataset, parameters['fgw']))\n",
        "    ia = sigmoid(np.matmul(concat_dataset, parameters['igw']))\n",
        "    oa = sigmoid(np.matmul(concat_dataset, parameters['ogw']))\n",
        "    ga = tanh(np.matmul(concat_dataset, parameters['ggw']))\n",
        "    cell_memory_matrix = np.multiply(fa, prev_cell_matrix) + np.multiply(ia, ga)\n",
        "    activation_matrix = np.multiply(oa, tanh(cell_memory_matrix))\n",
        "    lstm_activations = {}\n",
        "    lstm_activations['fa'] = fa\n",
        "    lstm_activations['ia'] = ia\n",
        "    lstm_activations['oa'] = oa\n",
        "    lstm_activations['ga'] = ga\n",
        "    return lstm_activations,cell_memory_matrix,activation_matrix\n",
        "\n",
        "\n",
        "def output_cell(activation_matrix, parameters):\n",
        "    output_matrix = softmax(np.matmul(activation_matrix, parameters['how'])) \n",
        "    return output_matrix\n",
        "\n",
        "\n",
        "def cal_loss_accuracy(batch_labels, output_cache):\n",
        "    loss, accuracy, prob = 0, 0, 1\n",
        "    batch_size = batch_labels[0].shape[0]\n",
        "    for i in range(1, len(output_cache)+1):\n",
        "        labels = batch_labels[i]\n",
        "        pred = output_cache['o' + str(i)]\n",
        "        prob = np.multiply(prob, np.sum(np.multiply(labels, pred), axis=1).reshape(-1, 1))\n",
        "        loss += np.sum((np.multiply(labels, np.log(pred)) + np.multiply(1-labels, np.log(1-pred))), axis=1).reshape(-1, 1)\n",
        "        accuracy += np.array(np.argmax(labels, 1) == np.argmax(pred, 1), dtype=np.float32).reshape(-1, 1)\n",
        "    perplexity = np.sum((1 / prob)**(1 / len(output_cache))) / batch_size\n",
        "    loss = np.sum(loss) * (-1 / batch_size)\n",
        "    accuracy = (np.sum(accuracy) / (batch_size)) / len(output_cache)\n",
        "    \n",
        "    return perplexity, loss, accuracy\n",
        "    \n",
        "\n",
        "def forward_propagation(batches, parameters, embeddings):\n",
        "    batch_size = batches[0].shape[0]\n",
        "    lstm_cache, activation_cache, cell_cache = {}, {}, {}\n",
        "    output_cache, embedding_cache = {}, {}\n",
        "    a0 = np.zeros([batch_size, HIDDEN], dtype=np.float32)\n",
        "    c0 = np.zeros([batch_size, HIDDEN], dtype=np.float32)\n",
        "    activation_cache['a0'] = a0\n",
        "    cell_cache['c0'] = c0\n",
        "    for i in range(len(batches) - 1):\n",
        "        batch_dataset = batches[i]\n",
        "        batch_dataset = get_embeddings(batch_dataset, embeddings)\n",
        "        embedding_cache['emb' + str(i)] = batch_dataset\n",
        "        lstm_activations, ct, at = lstm_cell(batch_dataset, a0, c0, parameters)\n",
        "        ot = output_cell(at, parameters)\n",
        "        lstm_cache['lstm' + str(i+1)]  = lstm_activations\n",
        "        activation_cache['a'+str(i+1)] = at\n",
        "        cell_cache['c' + str(i+1)] = ct\n",
        "        output_cache['o'+str(i+1)] = ot\n",
        "        a0 = at\n",
        "        c0 = ct  \n",
        "    return embedding_cache, lstm_cache, activation_cache, cell_cache, output_cache\n",
        "\n",
        "\n",
        "def calculate_output_cell_error(batch_labels, output_cache, parameters):\n",
        "    output_error_cache, activation_error_cache = {}, {}\n",
        "    for i in range(1, len(output_cache)+1):\n",
        "        error_output = output_cache['o' + str(i)] - batch_labels[i]\n",
        "        error_activation = np.matmul(error_output, parameters['how'].T)\n",
        "        output_error_cache['eo'+str(i)] = error_output\n",
        "        activation_error_cache['ea'+str(i)] = error_activation\n",
        "    return output_error_cache, activation_error_cache\n",
        "\n",
        "\n",
        "def calculate_single_lstm_cell_error(activation_output_error, next_activation_error,\n",
        "                                     next_cell_error, parameters, lstm_activation,\n",
        "                                     cell_activation, prev_cell_activation):\n",
        "    activation_error = activation_output_error + next_activation_error\n",
        "    oa = lstm_activation['oa']\n",
        "    ia = lstm_activation['ia']\n",
        "    ga = lstm_activation['ga']\n",
        "    fa = lstm_activation['fa']\n",
        "    eo = np.multiply(np.multiply(np.multiply(activation_error, tanh(cell_activation)), oa), 1-oa)\n",
        "    cell_error = np.multiply(np.multiply(activation_error, oa), dtanh(tanh(cell_activation)))\n",
        "    cell_error += next_cell_error\n",
        "    ei = np.multiply(np.multiply(np.multiply(cell_error, ga), ia), 1-ia)\n",
        "    eg = np.multiply(np.multiply(cell_error, ia), dtanh(ga))\n",
        "    ef = np.multiply(np.multiply(np.multiply(cell_error, prev_cell_activation), fa), 1-fa)\n",
        "    prev_cell_error = np.multiply(cell_error, fa)\n",
        "    embed_activation_error = np.matmul(ef, parameters['fgw'].T)\n",
        "    embed_activation_error += np.matmul(ei, parameters['igw'].T)\n",
        "    embed_activation_error += np.matmul(eo, parameters['ggw'].T)\n",
        "    embed_activation_error += np.matmul(eg, parameters['ogw'].T)\n",
        "    input_units = parameters['fgw'].shape[0] - parameters['fgw'].shape[1]\n",
        "    prev_activation_error = embed_activation_error[:, input_units:]\n",
        "    embed_error = embed_activation_error[:, :input_units]\n",
        "    lstm_error = {}\n",
        "    lstm_error['ef'] = ef\n",
        "    lstm_error['ei'] = ei\n",
        "    lstm_error['eo'] = eo\n",
        "    lstm_error['eg'] = eg\n",
        "    return prev_activation_error, prev_cell_error, embed_error, lstm_error\n",
        "\n",
        "\n",
        "def backward_propagation(batch_labels, embedding_cache, lstm_cache,\n",
        "                         activation_cache, cell_cache, output_cache, parameters):\n",
        "    output_error_cache, activation_error_cache = calculate_output_cell_error(batch_labels, output_cache, parameters)\n",
        "    lstm_error_cache, embedding_error_cache = {}, {}\n",
        "    eat = np.zeros(activation_error_cache['ea1'].shape)\n",
        "    ect = np.zeros(activation_error_cache['ea1'].shape)\n",
        "    for i in range(len(lstm_cache), 0, -1):\n",
        "        pae, pce, ee, le = calculate_single_lstm_cell_error(activation_error_cache['ea'+str(i)], eat, ect, parameters, lstm_cache['lstm'+str(i)], cell_cache['c'+str(i)], cell_cache['c'+str(i-1)])\n",
        "        lstm_error_cache['elstm'+str(i)] = le\n",
        "        embedding_error_cache['eemb'+str(i-1)] = ee\n",
        "        eat = pae\n",
        "        ect = pce\n",
        "    derivatives = {}\n",
        "    derivatives['dhow'] = calculate_output_cell_derivatives(output_error_cache, activation_cache, parameters)\n",
        "    lstm_derivatives = {}\n",
        "    for i in range(1, len(lstm_error_cache)+1):\n",
        "        lstm_derivatives['dlstm'+str(i)] = calculate_single_lstm_cell_derivatives(lstm_error_cache['elstm'+str(i)], embedding_cache['emb'+str(i-1)], activation_cache['a'+str(i-1)])\n",
        "    derivatives['dfgw'] = np.zeros(parameters['fgw'].shape)\n",
        "    derivatives['digw'] = np.zeros(parameters['igw'].shape)\n",
        "    derivatives['dogw'] = np.zeros(parameters['ogw'].shape)\n",
        "    derivatives['dggw'] = np.zeros(parameters['ggw'].shape)\n",
        "    for i in range(1, len(lstm_error_cache)+1):\n",
        "        derivatives['dfgw'] += lstm_derivatives['dlstm'+str(i)]['dfgw']\n",
        "        derivatives['digw'] += lstm_derivatives['dlstm'+str(i)]['digw']\n",
        "        derivatives['dogw'] += lstm_derivatives['dlstm'+str(i)]['dogw']\n",
        "        derivatives['dggw'] += lstm_derivatives['dlstm'+str(i)]['dggw']\n",
        "    return derivatives, embedding_error_cache\n",
        "\n",
        "\n",
        "def calculate_output_cell_derivatives(output_error_cache, activation_cache, parameters):\n",
        "    dhow = np.zeros(parameters['how'].shape)\n",
        "    batch_size = activation_cache['a1'].shape[0]\n",
        "    for i in range(1, len(output_error_cache)+1):\n",
        "        output_error = output_error_cache['eo' + str(i)]\n",
        "        activation = activation_cache['a'+str(i)]\n",
        "        dhow += np.matmul(activation.T,output_error)/batch_size\n",
        "    return dhow\n",
        "\n",
        "\n",
        "def calculate_single_lstm_cell_derivatives(lstm_error, embedding_matrix, activation_matrix):\n",
        "    concat_matrix = np.concatenate((embedding_matrix, activation_matrix), axis=1) \n",
        "    batch_size = embedding_matrix.shape[0]\n",
        "    derivatives = {}\n",
        "    derivatives['dfgw'] = np.matmul(concat_matrix.T, lstm_error['ef']) / batch_size\n",
        "    derivatives['digw'] = np.matmul(concat_matrix.T, lstm_error['ei']) / batch_size\n",
        "    derivatives['dogw'] = np.matmul(concat_matrix.T, lstm_error['eo']) / batch_size\n",
        "    derivatives['dggw'] = np.matmul(concat_matrix.T, lstm_error['eg']) / batch_size\n",
        "    return derivatives\n",
        "\n",
        "\n",
        "def update_parameters(parameters, derivatives, V, S):\n",
        "    vfgw = BETA1 * V['vfgw'] + (1 - BETA1) * derivatives['dfgw']\n",
        "    vigw = BETA1 * V['vigw'] + (1 - BETA1) * derivatives['digw']\n",
        "    vogw = BETA1 * V['vogw'] + (1 - BETA1) * derivatives['dogw']\n",
        "    vggw = BETA1 * V['vggw'] + (1 - BETA1) * derivatives['dggw']\n",
        "    vhow = BETA1 * V['vhow'] + (1 - BETA1) * derivatives['dhow']\n",
        "    sfgw = BETA2 * S['sfgw'] + (1 - BETA2) * derivatives['dfgw']**2\n",
        "    sigw = BETA2 *S['sigw'] + (1 - BETA2) * derivatives['digw']**2\n",
        "    sogw = BETA2 *S['sogw'] + (1 - BETA2) * derivatives['dogw']**2\n",
        "    sggw = BETA2 * S['sggw'] + (1 - BETA2) * derivatives['dggw']**2\n",
        "    show = BETA2 * S['show'] + (1 - BETA2) * derivatives['dhow']**2\n",
        "    parameters['fgw'] -= LEARNING_RATE * (vfgw / (np.sqrt(sfgw) + 1e-6))\n",
        "    parameters['igw'] -= LEARNING_RATE * (vigw / (np.sqrt(sigw) + 1e-6))\n",
        "    parameters['ogw'] -= LEARNING_RATE * (vogw / (np.sqrt(sogw) + 1e-6))\n",
        "    parameters['ggw'] -= LEARNING_RATE * (vggw / (np.sqrt(sggw) + 1e-6))\n",
        "    parameters['how'] -= LEARNING_RATE * (vhow / (np.sqrt(show) + 1e-6))\n",
        "    V['vfgw'], V['vigw'], V['vogw'], V['vggw'], V['vhow'] = vfgw, vigw, vogw, vggw, vhow\n",
        "    S['sfgw'], S['sigw'], S['sogw'], S['sggw'], S['show'] = sfgw, sigw, sogw, sggw, show\n",
        "    return parameters, V, S\n",
        "\n",
        "\n",
        "def update_embeddings(embeddings, embedding_error_cache, batch_labels):\n",
        "    embedding_derivatives = np.zeros(embeddings.shape)\n",
        "    batch_size = batch_labels[0].shape[0]\n",
        "    for i in range(len(embedding_error_cache)):\n",
        "        embedding_derivatives += np.matmul(batch_labels[i].T, embedding_error_cache['eemb'+str(i)]) / batch_size\n",
        "    embeddings = embeddings - LEARNING_RATE * embedding_derivatives\n",
        "    return embeddings\n",
        "\n",
        "\n",
        "# train ######################################################################\n",
        "    \n",
        "# get batched dataset\n",
        "def get_batches(songs, data_int):\n",
        "    train_dataset = []\n",
        "    for i in range(len(songs) - BATCH_SIZE + 1):\n",
        "        start = i * BATCH_SIZE\n",
        "        end = start + BATCH_SIZE\n",
        "        batch_data = songs[start:end]\n",
        "        if(len(batch_data) != BATCH_SIZE):\n",
        "            break\n",
        "        note_list = []\n",
        "        for j in range(len(batch_data[0])):\n",
        "            batch_dataset = np.zeros([BATCH_SIZE, len(data_int)])\n",
        "            for k in range(BATCH_SIZE):\n",
        "                note = batch_data[k][j]\n",
        "                idx = data_to_int[note]\n",
        "                batch_dataset[k, idx] = 1\n",
        "            note_list.append(batch_dataset)\n",
        "        train_dataset.append(note_list)\n",
        "    return train_dataset\n",
        "\n",
        "def train(train_dataset, iters):\n",
        "    # initalize the parameters\n",
        "    parameters = initialize_parameters()\n",
        "    # initialize the parameters for Adam optimizer\n",
        "    V = initialize_V(parameters)\n",
        "    S = initialize_S(parameters)\n",
        "    # generate the random embeddings\n",
        "    embeddings = np.random.normal(0, 0.01, (vocab_size, INPUT))\n",
        "    # Loss, Perplexity and Accuracy for each batch\n",
        "    L, P, A = [], [], []\n",
        "    for step in range(iters):\n",
        "        # get batch dataset\n",
        "        index = step % len(train_dataset)\n",
        "        batches = train_dataset[index]\n",
        "        # forward propagation\n",
        "        embedding_cache, lstm_cache, activation_cache, cell_cache, output_cache = forward_propagation(batches, parameters, embeddings)\n",
        "        # calculate the loss, perplexity and accuracy\n",
        "        perplexity, loss, acc = cal_loss_accuracy(batches, output_cache)\n",
        "        # backward propagation\n",
        "        derivatives, embedding_error_cache = backward_propagation(batches, embedding_cache, lstm_cache, activation_cache, cell_cache, output_cache, parameters) \n",
        "        # update the parameters\n",
        "        parameters, V, S = update_parameters(parameters, derivatives, V, S)\n",
        "        # update the embeddings\n",
        "        embeddings = update_embeddings(embeddings, embedding_error_cache, batches)\n",
        "        # print error measures every 100 epochs\n",
        "        L.append(loss)\n",
        "        P.append(perplexity)\n",
        "        A.append(acc)\n",
        "        if(step % 100 == 0):\n",
        "            print(\"For Single Batch :\")\n",
        "            print('Step       = {}'.format(step))\n",
        "            print('Loss       = {}'.format(round(loss,2)))\n",
        "            print('Perplexity = {}'.format(round(perplexity,2)))\n",
        "            print('Accuracy   = {}'.format(round(acc*100,2)))\n",
        "            print()\n",
        "    return embeddings, parameters, L, P, A\n",
        "\n",
        "train_set = get_batches(songs_data, data_to_int)\n",
        "embeddings, parameters, L, P, A = train(train_set, 1000)\n",
        "\n",
        "# plot\n",
        "avg_loss, avg_acc, avg_perp = [], [], []\n",
        "i = 0\n",
        "while(i < len(L)):\n",
        "    avg_loss.append(np.mean(L[i:i+50]))\n",
        "    avg_acc.append(np.mean(A[i:i+50]))\n",
        "    avg_perp.append(np.mean(P[i:i+50]))\n",
        "    i += 50\n",
        "\n",
        "plt.plot(list(range(len(avg_loss))), avg_loss)\n",
        "plt.xlabel(\"Iteration of 50 batches\")\n",
        "plt.ylabel(\"Average Loss\")\n",
        "plt.title(\"Average Loss of Each 50 Batches\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(list(range(len(avg_perp))), avg_perp)\n",
        "plt.xlabel(\"Iteration of 50 batches\")\n",
        "plt.ylabel(\"Average Perplexity\")\n",
        "plt.title(\"Average Perplexity of Each 50 Batches\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(list(range(len(avg_acc))), avg_acc)\n",
        "plt.xlabel(\"Iteration of 50 batches\")\n",
        "plt.ylabel(\"Average Accuracy\")\n",
        "plt.title(\"Average Accuracy of Each 50 Batches\")\n",
        "plt.show()\n",
        "\n",
        "# PART.3 #####################################################################\n",
        "# predict data from random initial value using the trained LSTM\n",
        "\n",
        "def predict(parameters, embeddings, idx2note, vocab_size):\n",
        "    out_notes = []    \n",
        "    # produce 10 pieces of music\n",
        "    for i in range(10):\n",
        "        a0 = np.zeros([1, HIDDEN], dtype=np.float32)\n",
        "        c0 = np.zeros([1, HIDDEN], dtype=np.float32)\n",
        "        notes = []\n",
        "        batch_dataset = np.zeros([1, vocab_size])\n",
        "        # get random start note\n",
        "        index = np.random.randint(0, vocab_size, 1)[0]     \n",
        "        batch_dataset[0, index] = 1.0\n",
        "        # add first note to the generating piece\n",
        "        notes.append(idx2note[index])\n",
        "        # get actual note from idx2note dict\n",
        "        note = idx2note[index]\n",
        "        while(note != (None, None, None, None)):\n",
        "            # get embeddings\n",
        "            batch_dataset = get_embeddings(batch_dataset, embeddings)\n",
        "            # lstm cell\n",
        "            lstm_activations, ct, at = lstm_cell(batch_dataset, a0, c0, parameters)\n",
        "            # output cell\n",
        "            ot = output_cell(at, parameters)\n",
        "            # select random.choice with output distribution!\n",
        "            # this helps eliminating repetition and gives more diversity\n",
        "            # better result than pred = np.argmax(ot)\n",
        "            pred = np.random.choice(vocab_size, 1, p=ot[0])[0]         \n",
        "            # add note to song\n",
        "            notes.append(idx2note[pred])\n",
        "            note = idx2note[pred]\n",
        "            # change the batch_dataset to this new predicted note\n",
        "            batch_dataset = np.zeros([1, vocab_size])\n",
        "            batch_dataset[0, pred] = 1.0\n",
        "            # update new 'at' and 'ct' for next lstm cell\n",
        "            a0 = at\n",
        "            c0 = ct\n",
        "        out_notes.append(notes)\n",
        "    return out_notes\n",
        "\n",
        "pred_notes = predict(parameters, embeddings, int_to_data, vocab_size)\n",
        "\n",
        "# write the generated songs as midi files\n",
        "count = 0\n",
        "for piece in pred_notes:\n",
        "    p1 = stream.Part()\n",
        "    p1.insert(0, instrument.Piano())\n",
        "    p2 = stream.Part()\n",
        "    p2.insert(0, instrument.Piano())\n",
        "    for item in piece:\n",
        "        if item != (None, None, None, None):\n",
        "            if item[0] != None and item[1] != None:\n",
        "                if '.' in item[0]:\n",
        "                    chord_pitches = item[0].split('.')\n",
        "                    p1.append(chord.Chord(chord_pitches, quarterLength = item[1]))\n",
        "                else:\n",
        "                    p1.append(note.Note(item[0], quarterLength = item[1]))\n",
        "            if item[2] != None and item[3] != None:\n",
        "                if '.' in item[2]:\n",
        "                    chord_pitches = item[2].split('.')\n",
        "                    p2.append(chord.Chord(chord_pitches, quarterLength = item[3]))\n",
        "                else:\n",
        "                    p2.append(note.Note(item[2], quarterLength = item[3]))\n",
        "    s = stream.Stream([p1, p2])\n",
        "    mid = midi.translate.streamToMidiFile(s)\n",
        "    mid.open('out'+str(count)+'.mid', 'wb')\n",
        "    mid.write()\n",
        "    mid.close()\n",
        "    print(\"saved generated song! check your directory.\")\n",
        "    count += 1"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}